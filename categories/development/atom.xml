<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: development | JP Simard]]></title>
  <link href="https://jpsim.com/categories/development/atom.xml" rel="self"/>
  <link href="https://jpsim.com/"/>
  <updated>2026-01-02T19:54:14+00:00</updated>
  <id>https://jpsim.com/</id>
  <author>
    <name><![CDATA[JP Simard]]></name>
    <email><![CDATA[jp@jpsim.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Mobile Development in the Age of AI]]></title>
    <link href="https://jpsim.com/mobile-development-in-the-age-of-ai/"/>
    <updated>2026-01-02T15:00:00+00:00</updated>
    <id>https://jpsim.com/mobile-development-in-the-age-of-ai</id>
    <content type="html"><![CDATA[<p>After working in mobile engineering for about a decade and a half, across
multiple eras of iOS and Android tooling, languages, and architectural
approaches, I've never been more excited about the landscape than I am right
now.</p>

<p>When I started making apps in 2011, it was thrilling to go from an idea to
something you could physically touch and hold in your hand within hours. Today,
that same feeling exists‚Äîbut the iteration cycles are now an order of magnitude
faster.</p>

<p>The fundamentals haven‚Äôt changed. What <em>has</em> changed is the speed at which
intent turns into working software.</p>

<h2>Translation Is (Basically) a Solved Problem</h2>

<p>A lot of where AI applications go wrong today comes from anthropomorphizing
models and ascribing to them a kind of human-style intelligence. As
<a href="https://www.youtube.com/watch?v=lXUZvyajciY">Andrej Karpathy</a> puts it: <em>‚ÄúWe‚Äôre summoning ghosts, not
building animals.‚Äù</em></p>

<p>But one category of tasks these LLM ‚Äúghosts‚Äù excel at is translation.</p>

<p>That‚Äôs obvious when translating natural language like English to French, but
it‚Äôs equally true for:</p>

<ul>
<li>TypeScript to Swift</li>
<li>Jetpack Compose to SwiftUI</li>
<li>Web APIs to platform SDKs</li>
<li>Architectural patterns across platforms</li>
</ul>


<p>See <a href="https://simonwillison.net/2025/Dec/15/porting-justhtml/">this example</a>
where Simon Willison ported JustHTML from Python to JavaScript with Codex CLI
and GPT-5.2 in 4.5 hours.</p>

<p>If you operate in an environment where your web app, iOS app, and Android app
are separate codebases with baseline functional parity, AI fundamentally changes
the economics of one codebase for each platform.</p>

<p>Instead of rewriting features, you‚Äôre translating intent.</p>

<p>A web PR becomes a reference implementation. Existing mobile code becomes
executable context. Asking an agent to ‚Äútake a first pass at implementing this
natively‚Äù is now a normal‚Äîand productive‚Äîworkflow, especially for apps with
large CRUD-style surface areas and well-defined business logic.</p>

<h2>Context Beats Code</h2>

<p><a href="https://simonwillison.net/2025/jun/27/context-engineering/">Simon Willison</a> has written about <em>context engineering</em>:
‚Äúthe art of providing all the context for the task to be plausibly solvable by
the LLM.‚Äù The limiting factor in AI-assisted development isn‚Äôt the model itself,
but how clearly the problem is framed.</p>

<p>That aligns closely with what actually matters in software engineering:</p>

<ul>
<li>Understanding the domain</li>
<li>Choosing the right abstractions</li>
<li>Designing the architecture</li>
<li>Encoding business rules correctly</li>
<li>Setting up the right conditions for QA and validating core behavior and edge cases</li>
</ul>


<p>These are platform-agnostic, one-time costs. Once you‚Äôve paid them, the knowledge
transfers cleanly across platforms.</p>

<p>This is also where the distinction between <strong>process-driven</strong> and
<strong>outcome-driven</strong> engineers becomes important. As
<a href="https://simonwillison.net/2026/Jan/2/ben-werdmuller/">Ben Werdmuller</a> put it, we‚Äôre
likely to see a real split between people who are outcome-driven and excited to
test their work with users faster, and people who are process-driven and derive
meaning primarily from the engineering itself.</p>

<p>LLMs disproportionately benefit outcome-driven engineers‚Äîthose who know what
‚Äúgood‚Äù looks like and can steer toward it. The tools reward clarity of intent,
not ceremony.</p>

<h2>What Mobile Development Actually Looks Like Now</h2>

<p>In practice, most of the code I‚Äôve written over the past twelve months hasn‚Äôt
been typed directly into Xcode or Android Studio.</p>

<p>A common setup today looks like this:</p>

<ul>
<li>A coding agent (e.g. Claude Code, Cursor, etc.) on one side of the screen</li>
<li>Xcode or Android Studio on the other</li>
<li>The bulk of code editing happening in the agent</li>
<li>The IDE acting as a platform-native harness: breakpoints, debugger, previews,
and deeper integrations with simulators and emulators</li>
</ul>


<p>With a modest amount of investment, the need for an IDE as a primary input tool
almost disappears. The more you can hoist your compiler and language server out
of the IDE and into terminal-ready commands, the more you can augment the
capabilities of your coding agent.</p>

<h2>Native Platforms Pair Well With AI, Despite Training Gaps</h2>

<p>A fair concern is that frontier models still have less deep knowledge of
iOS- and Android-specific APIs than they do of web or Python ecosystems.</p>

<p><a href="https://www.cocoawithlove.com/blog/llms-twelve-months-later.html">Matt Gallagher</a> recently wrote about the state of Swift and iOS
knowledge in frontier models at the end of 2025. He notes that <em>‚Äúmost LLMs are
trained on data that is 2+ years old, and their Swift style often feels older
still.‚Äù</em></p>

<p>The conclusion isn‚Äôt that models are perfect, but that modern native platforms
provide strong constraints:</p>

<ul>
<li>Strong static typing</li>
<li>Clear compiler diagnostics</li>
<li>Declarative UI frameworks</li>
<li>Deterministic tooling</li>
</ul>


<p>These constraints matter more than raw training data volume. What matters day to
day isn‚Äôt whether an LLM can one-shot a perfect solution, but whether it can
iterate quickly with tight feedback. Strong typing in Swift, Kotlin, and
TypeScript helps enormously here.</p>

<p>I don‚Äôt know about you, but I don‚Äôt typically write perfect code on the first try
either.</p>

<h2>A Brief Note on React Native</h2>

<p>It‚Äôs absolutely possible to build excellent mobile apps using React Native or
fully native stacks.</p>

<p>What‚Äôs often underestimated is the organizational overhead and depth of
expertise required to do React Native well at scale. Teams that succeed tend to
invest heavily in platform infrastructure, tooling, and internal knowledge.
Shopify is a well-known example of what ‚Äúdoing it right‚Äù looks like.</p>

<p>AI shifts this tradeoff. When translation becomes cheap, the value of
lowest-common-denominator abstractions drops. Platform-idiomatic code no longer
implies slower delivery.</p>

<p>Side note: this is a large part of why Ramp‚Äôs mobile engineering team is much
leaner than people expect given the breadth of the apps' capabilities.</p>

<h2>The Centaur Model, Applied</h2>

<p>This way of working maps closely to what Cory Doctorow calls the
<a href="https://pluralistic.net/2025/12/05/pop-that-bubble/#u-washington"><em>Centaur model</em></a>. In automation theory, ‚Äúa centaur‚Äù is a person
assisted by a machine.</p>

<p>Humans and machines work together, each focused on what they do best:</p>

<ul>
<li>Humans handle judgment, architecture, domain understanding, and taste</li>
<li>Machines handle repetition, translation, and acceleration</li>
</ul>


<p>AI doesn‚Äôt remove the need for software engineers. It removes the least
interesting parts of the job and sharpens the most important ones.</p>

<h2>Why This Reinforces Native Teams</h2>

<p>With AI-assisted translation:</p>

<ul>
<li>Separate codebases no longer incur the same maintenance costs as they once did</li>
<li>Platform fidelity no longer trades off against velocity</li>
<li>Engineers can build where they‚Äôre strongest and port confidently</li>
</ul>


<p>This is exactly how we‚Äôre working today‚Äîand why Ramp is hiring <a href="https://jobs.ashbyhq.com/ramp/4859cd5e-f2a9-44d7-81f7-8bfc0e62369f">iOS engineers</a>
and <a href="https://jobs.ashbyhq.com/ramp/f564dcf9-9390-4a3f-896f-8047a5086040">Android engineers</a>.</p>

<p>If you care about building platform-idiomatic mobile apps, moving quickly
without sacrificing quality, and spending more time on judgment than
boilerplate, this is a particularly good moment to be doing native mobile
engineering.</p>

<h2>Closing</h2>

<p>The tools changed.<br/>
The job didn‚Äôt.</p>

<p>If anything, it got more interesting.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Building an NFC Music Box]]></title>
    <link href="https://jpsim.com/building-an-nfc-music-box/"/>
    <updated>2023-03-31T11:00:00+00:00</updated>
    <id>https://jpsim.com/building-an-nfc-music-box</id>
    <content type="html"><![CDATA[<p>My son and I built an NFC-based music player so he can play the music he
wants, develop his own musical taste but most of all so that he and I
could have something fun to build together.</p>

<p>Kind of like a modern day record player, but infinitely more extensible,
and with each album costing 1/100th the price of a vinyl record.</p>

<p>Here it is in action (<em>music starts 9 seconds into the video, set your
volume to low</em>):</p>

<iframe title="vimeo-player" src="https://player.vimeo.com/video/813573763?h=6b3ebec0bc" width="640" height="360" frameborder="0" allowfullscreen></iframe>


<p><br /></p>

<p>And here's how we built it:</p>

<h2>Materials</h2>

<table>
<thead>
<tr>
<th><br /></th>
<th><br /></th>
<th><br /></th>
<th><br /></th>
<th><br /></th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="/images/posts/musicbox/echo.jpg" alt="" /></td>
<td><img src="/images/posts/musicbox/case.jpg" alt="" /></td>
<td><img src="/images/posts/musicbox/pi.jpg" alt="" /></td>
<td><img src="/images/posts/musicbox/nfc_module.jpg" alt="" /></td>
<td><img src="/images/posts/musicbox/nfc_cards.jpg" alt="" /></td>
</tr>
</tbody>
</table>


<ul>
<li><a href="https://www.amazon.com/dp/B08YT3BWMP">Amazon Echo Dot, $20</a>:
For the price, this is a surprisingly hackable, portable speaker. I would have
prefered something without any microphones at all, but I <em>mostly</em> trust the
hardware mute switch.</li>
<li><a href="https://www.etsy.com/listing/739034156">Case, $20</a>: Beautiful
retro-style case for the Raspberry Pi. The top compartment is made to
house a fan, but this is where I put the NFC module instead. The folks
who make this at <a href="https://twitter.com/theC4Labs">C4 Labs</a> were <a href="https://twitter.com/simjp/status/1259980961665576960">super nice</a>.
We broke a piece when we started building the box and they sent a
replacement part right away.</li>
<li><a href="https://www.raspberrypi.org/products/raspberry-pi-4-model-b/">Raspberry Pi, $35</a>:
I chose to go with a Raspberry Pi because they're cheap, have good
compatible NFC modules, lots of case options on Etsy and easy to
develop on.</li>
<li><a href="https://www.amazon.com/dp/B01CSTW0IA">NFC Module, $5</a>: I can't
believe this thing is just $5. Works great. Lots of good tutorials on
connecting this to a Raspberry Pi.</li>
<li><a href="https://store.gototags.com/nfc-pvc-card-ntag213/">NFC Cards, $29</a>:
I got 100 cards at $0.29 each.</li>
</ul>


<p>In total, this adds up to $109, but in my case I already had a Raspberry
Pi and an Echo Dot lying around that I could repurpose for this, so it
cost me closer to $55.</p>

<h2>Services</h2>

<p>I'm already an Apple Music subscriber, so using that as the music source
was the cheapest solution, although this setup would work with any music
service that works with Amazon Echo: Spotify, Amazon Music, TuneIn,
CloudPlayer, Deezer, iHeartRadio.</p>

<p>In the exploration phase for this project, I wanted to buy DRM-free
music, wire a speaker directly to the Raspberry Pi and play it locally.
However, that would have meant that adding music later would be a much
more tedious task. Plus it's hard to find DRM-free music these days, and
when you do find what you want it ends up being pricey. Especially
compared to the convenience and cost of today's streaming options.</p>

<p>I also explored using HomePods as the speakers (see addendum below).</p>

<p>The biggest downside to the current streaming approach is that there's a
5-10 second delay after tapping a card and music starting.</p>

<h2>Assembly</h2>

<table>
<thead>
<tr>
<th><br /></th>
<th><br /></th>
<th><br /></th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="/images/posts/musicbox/assembly1.jpg" alt="Assembly #1" /></td>
<td><img src="/images/posts/musicbox/assembly2.jpg" alt="Assembly #2" /></td>
<td><img src="/images/posts/musicbox/assembly3.jpg" alt="Assembly #3" /></td>
</tr>
<tr>
<td><img src="/images/posts/musicbox/assembly4.jpg" alt="Assembly #4" /></td>
<td><img src="/images/posts/musicbox/assembly5.jpg" alt="Assembly #5" /></td>
<td><img src="/images/posts/musicbox/assembly6.jpg" alt="Assembly #6" /></td>
</tr>
</tbody>
</table>


<p><br /></p>

<p>I assembled this with my son over a few days in May 2020, and then
finished the setup in September 2021. The actual hardware assembly
probably only took a combined total of 3 hours though.</p>

<h2>Software</h2>

<p><em>Disclaimer: This code is rough, it's suitable for a toy project. I'm
sharing it in case it's useful for others getting started with a similar,
project. I'm not claiming this is beautiful quality code.</em></p>

<h3>Echo Dot Remote Control</h3>

<p>I use the <a href="https://github.com/thorsten-gehrig/alexa-remote-control">alexa-remote-control</a> shell script to
control the Echo Dot. When it's up and running, it couldn't be easier:</p>

<pre><code class="shell">$ alexa_remote_control.sh -e "playmusic:APPLE_MUSIC:The Lion King"
$ alexa_remote_control.sh -e pause
$ alexa_remote_control.sh -e play
$ alexa_remote_control.sh -e vol:15
</code></pre>

<h3>NFC Reader</h3>

<p>I use <a href="https://github.com/pelwell/MFRC522-python">MFRC522-python</a> to
interface with the NFC module.</p>

<p>The Python script that reads cards and plays music looks like this:</p>

<pre><code class="python">import RPi.GPIO as GPIO
import MFRC522
import os
import signal

continue_reading = True
last_seen_card = None
echo_name = "MusicBox Echo"
current_dir = os.path.dirname(os.path.abspath(__file__))
alexa_control_script = os.path.join(current_dir, "alexa_remote_control.sh")

def alexa(command):
    os.system(
        '{alexa_control_script} -d "{echo_name}" -e "{command}"'.format(
            alexa_control_script=alexa_control_script,
            echo_name=echo_name,
            command=command,
        )
    )

def pause():
    alexa("pause")

def play_music(query):
    print(query)
    pause()
    alexa("playmusic:APPLE_MUSIC:{query}".format(query=query))

def set_volume(volume):
    alexa("vol:{volume}".format(volume=volume))

def end_read(signal, frame):
    global continue_reading
    continue_reading = False
    GPIO.cleanup()

signal.signal(signal.SIGINT, end_read)

MIFAREReader = MFRC522.MFRC522()

while continue_reading:
    # Scan for cards
    (status, TagType) = MIFAREReader.MFRC522_Request(MIFAREReader.PICC_REQIDL)

    # If we have a card, continue
    if status == MIFAREReader.MI_OK:
        # Get the UID of the card
        (status, uid) = MIFAREReader.MFRC522_Anticoll()

        # If we have the UID, continue
        if status == MIFAREReader.MI_OK and last_seen_card != uid:
            last_seen_card = uid

            # Print UID
            print("Card UID: %s" % uid)
            if uid == [136, 4, 76, 240, 48]:
                print("PAUSE")
                pause()
            elif uid == [136, 4, 236, 236, 140]:
                set_volume(10)
            elif uid == [136, 4, 247, 42, 81]:
                set_volume(40)
            elif uid == [136, 4, 137, 170, 175]:
                play_music("Star Wars A New Hope Soundtrack")
            elif uid == [136, 4, 148, 191, 167]:
                play_music("Paco de Lucia")
            elif uid == [136, 4, 160, 224, 204]:
                play_music("Dexter Gordon")
            elif uid == [136, 4, 197, 238, 167]:
                play_music("Rainbow Connection")
            elif uid == [136, 4, 2, 220, 82]:
                play_music("Genesis")
            elif uid == [136, 4, 232, 57, 93]:
                # and so on...
</code></pre>

<p>You'll notice that there are special cards for pausing, setting a high
volume and setting a low volume. The rest of the cards map to music.</p>

<p>The main thing I'd like to improve at some point is to avoid hardcoding
a mapping of the card UIDs to music and instead program it using the
companion iPhone app by writing a payload to the card. The iOS side I
know how to do pretty quickly, but I'd have to spend more time to figure
out how to do it on the MFRC522 reader side of things.</p>

<h3>API Server</h3>

<p>There's an API server that can be used to control the music box using
the companion iPhone app.</p>

<pre><code class="python">from flask import Flask, request, jsonify
import os

api = Flask(__name__)
echo_name = "MusicBox Echo"
current_dir = os.path.dirname(os.path.abspath(__file__))
alexa_control_script = os.path.join(current_dir, "alexa_remote_control.sh")

def alexa(command):
    os.system(
        '{alexa_control_script} -d "{echo_name}" -e "{command}"'.format(
            alexa_control_script=alexa_control_script,
            echo_name=echo_name,
            command=command,
        )
    )

@api.route("/pause", methods=["POST"])
def pause():
    alexa("pause")
    return "{}"

@api.route("/play", methods=["POST"])
def play():
    if request.json:
        play_music(request.json["query"])
    else:
        alexa("play")
    return "{}"

@api.route("/play/&lt;query&gt;", methods=["POST"])
def play_music(query):
    print(query)
    pause
    alexa("playmusic:APPLE_MUSIC:{query}".format(query=query))
    return "{}"

@api.route("/volume/&lt;int:volume&gt;", methods=["POST"])
def set_volume(volume):
    alexa("vol:{volume}".format(volume=volume))
    return "{}"

if __name__ == "__main__":
    api.run(host="0.0.0.0")
</code></pre>

<h3>iPhone App</h3>

<p>Of course I made an iPhone app using <a href="https://developer.apple.com/xcode/swiftui/">SwiftUI</a> &amp;
<a href="https://github.com/pointfreeco/swift-composable-architecture">Composable Architecture</a>.</p>

<p>This helps me quickly adjust the volume, play/pause and play specific
music. If my kid falls alseep to music, I can stop it without having to
walk into his room.</p>

<p>Siri intents work too, so I can play/pause music by speaking to Siri
without having to launch the app.</p>

<p><img src="/images/posts/musicbox/app.jpg" alt="" /></p>

<h2>Closed Source</h2>

<p>Hopefully this post is helpful to someone interested in building
something similar. I've posted enough code and details to get you
started, but I won't be open sourcing the whole project because I'm just
not willing to field support questions or feature requests. I'm happy to
answer questions about my experience building this, but I'm sorry I
can't help you figure out why something's not working if you go build
something similar.</p>

<hr />

<h2>Addendum: HomePod Attempt</h2>

<p>At one point I wanted to use a stereo pair of HomePods for this project,
but all my attempts to reverse engineer a way to play music on them from
a Raspberry Pi were fruitless. I tried sniffing the network traffic via
Charles Proxy while playing music from the iOS Music app or even the
Shortcuts app and wasn't able to crack it. I did end up getting it
working using <a href="https://github.com/owntone/owntone-server">forked-daap</a> but this set the HomePods in a
weird state. I don't remember all the details because I gave up on that
approach for two reasons: the first is that I couldn't get it to work
well and the second is that I wanted to keep the HomePods in our living
room while the music box was meant to be in my son's bedroom.</p>

<hr />
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Evaluating SwiftSyntax for use in SwiftLint]]></title>
    <link href="https://jpsim.com/evaluating-swiftsyntax-for-use-in-swiftlint/"/>
    <updated>2018-11-22T14:50:00+00:00</updated>
    <id>https://jpsim.com/evaluating-swiftsyntax-for-use-in-swiftlint</id>
    <content type="html"><![CDATA[<p><strong>tl;dr; Implementing SwiftLint using SwiftSyntax instead of SourceKitten would make it run over 20x slower üò≠</strong></p>

<p><strong>Update:</strong> Since writing this post, I learnt that SwiftSyntax's upcoming byte tree deserialization mode will speed this up considerably.
I hope to post a follow-up article on this shortly.</p>

<p>I have for some time been looking forward to reimplementing some of <a href="https://github.com/realm/SwiftLint">SwiftLint</a>'s simpler syntax-only rules with <a href="https://github.com/apple/swift-syntax">SwiftSyntax</a>. If you're not familiar with it, the recent <a href="https://nshipster.com/swiftsyntax/">NSHipster article</a> gives a great overview. My motivation for integrating it into SwiftLint was that it would be nice to use an officially maintained library directly to obtain the syntax tree rather than the open source but community-maintained <a href="https://github.com/jpsim/SourceKitten">SourceKitten</a> library. I was also under the false impression that SwiftSyntax would be significantly faster than SourceKit/SourceKitten.</p>

<p>SourceKitten gets its syntax tree by dynamically loading <a href="https://github.com/apple/swift/tree/master/tools/SourceKit">SourceKit</a> and making cross-process XPC calls to a SourceKit daemon. In a typical uncached lint run, SwiftLint spends a significant amount of time waiting on this syntax tree for each file being linted. Because SwiftSyntax is <a href="https://github.com/apple/swift-syntax#building-swiftsyntax-from-master">code-generated</a> from the same syntax definition files as the Swift compiler, I had (incorrectly) assumed that calculating a Swift file's syntax tree using SwiftSyntax was done entirely in-process by the library, which would have lead to significant performance gains by avoiding the cross-process XPC call made by SourceKitten for equivalent functionality.</p>

<p>In reality, SwiftSyntax delegates all parsing &amp; lexing to the <code>swiftc</code> binary, <a href="https://github.com/apple/swift-syntax/blob/0.40200.0/Sources/SwiftSyntax/SwiftSyntax.swift#L100-L101">launching the process</a>, <a href="https://github.com/apple/swift-syntax/blob/0.40200.0/Sources/SwiftSyntax/SwiftSyntax.swift#L102">reading its output from stdout</a> and <a href="https://github.com/apple/swift-syntax/blob/0.40200.0/Sources/SwiftSyntax/SwiftSyntax.swift#L103-L104">deserializing the JSON response</a> into its <code>SourceFileSyntax</code> Swift type. This is repeated for each file being parsed üò±.</p>

<p><strong>Launching a new instance of the Swift compiler for each file parsed is orders of magnitude slower than SourceKitten's XPC call to a long-lived SourceKit daemon.</strong></p>

<p>I discovered this after <a href="https://github.com/realm/SwiftLint/pull/2476">reimplementing</a> a very simple SwiftLint rule with a SwiftSyntax-based implementation: <a href="https://github.com/realm/SwiftLint/blob/master/Rules.md#fallthrough">Fallthrough</a>. This opt-in rule is a perfect proof-of-concept for integrating SwiftSyntax into SwiftLint because it literally just finds all occurrences of the <code>fallthrough</code> keyword and reports a violation at that location. I measured the time it took to lint a folder of ~100 Swift files from Lyft's iOS codebase with only the <code>fallthrough</code> rule whitelisted.</p>

<pre><code class="yaml"># .swiftlint.yml configuration file
included:
  - path/to/lint/dir # contains ~100 Swift files
whitelist_rules:
  - fallthrough
</code></pre>

<p>I compiled both SwiftLint from <code>master</code> and again with this <code>fallthrough-swift-syntax</code> branch with <code>swift build -c release</code> and named the binaries <code>swiftlint-master</code> and <code>swiftlint-swift-syntax</code>. I then benchmarked both binaries using the excellent <a href="https://github.com/sharkdp/hyperfine">hyperfine</a> utility.</p>

<pre><code class="shell">$ hyperfine './swiftlint-master lint --quiet --no-cache' './swiftlint-swift-syntax lint --quiet --no-cache'
Benchmark #1: ./swiftlint-master lint --quiet --no-cache
  Time (mean ¬± œÉ):     231.3 ms ¬±   5.1 ms    [User: 130.5 ms, System: 29.2 ms]
  Range (min ‚Ä¶ max):   224.3 ms ‚Ä¶ 238.3 ms

Benchmark #2: ./swiftlint-swift-syntax lint --quiet --no-cache
  Time (mean ¬± œÉ):      5.254 s ¬±  0.149 s    [User: 20.309 s, System: 23.110 s]
  Range (min ‚Ä¶ max):    4.839 s ‚Ä¶  5.354 s

Summary
  './swiftlint-master lint --quiet --no-cache' ran
   22.71 ¬± 0.82 times faster than './swiftlint-swift-syntax lint --quiet --no-cache'
</code></pre>

<p><strong>The SwiftSyntax version was 22x slower than the existing SourceKitten version</strong></p>

<p><em>Note that I ran SwiftLint with its caching mechanism and logging disabled to accurately measure the time it took just to perform the lint, rather than the overhead from logging or skipping the lint entirely by just returning cached results. Although logging only added 3ms to 10ms in my tests.</em></p>

<hr />

<p>Ultimately, this means SwiftLint will be keeping its SourceKitten-based implementation for the foreseeable future, unless SwiftSyntax removes its reliance on costly compiler invocations and drastically improves its performance. I really hope the Swift team can somehow find a way to move parsing and lexing into SwiftSyntax itself, making the library much more appealing to use.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Introducing Yams 1.0]]></title>
    <link href="https://jpsim.com/introducing-yams-1-dot-0/"/>
    <updated>2018-05-19T11:40:00+00:00</updated>
    <id>https://jpsim.com/introducing-yams-1-dot-0</id>
    <content type="html"><![CDATA[<p><img src="https://raw.githubusercontent.com/jpsim/Yams/master/yams.jpg" alt="Yams: A Sweet &amp; Swifty YAML Parser" /></p>

<p><a href="https://twitter.com/norio_nomura">Norio Nomura</a> and I (ok, honestly mostly Norio üòÖ) have been building a
Swift library for encoding &amp; decoding <a href="http://yaml.org">YAML</a> for the last 18 months and
it's now stable enough to make a 1.0 release and share with the world.</p>

<p>It's called Yams, you can find it on GitHub at <a href="https://github.com/jpsim/Yams">jpsim/Yams</a> and API
docs are located at <a href="https://jpsim.com/Yams">jpsim.com/Yams</a>.</p>

<p>You could say it's a Swift binding for <a href="https://github.com/yaml/libyaml">LibYAML</a> but I'd argue it's
much more than that.</p>

<p>I'm actually very happy with how this library ended up. It plays nicely with
Swift 4's <a href="https://developer.apple.com/documentation/foundation/archives_and_serialization/encoding_and_decoding_custom_types">Codable protocol</a>, meaning that you get fast &amp; type-safe
encoding &amp; decoding that feels right at home in modern Swift.</p>

<p>Here's a simple example of encoding &amp; decoding <code>Codable</code> types:</p>

<pre><code class="swift">import Yams

struct S: Codable {
  var p: String
}

let s = S(p: "test")
let encoder = YAMLEncoder()
let encodedYAML = try encoder.encode(s)
encodedYAML == """
  p: test

  """
let decoder = YAMLDecoder()
let decoded = try decoder.decode(S.self, from: encodedYAML)
s.p == decoded.p
</code></pre>

<p>Alternatively, you can use it to work with Swift scalar &amp; collection types,
which is probably the easiest way to start parsing arbitrary YAML for your
projects. Finally, there's a third mode, which is a Yams-native API that best
translates to how LibYAML works.</p>

<p>This library's been powering a number of popular projects that use YAML for
configuration, like <a href="https://github.com/realm/SwiftLint">SwiftLint</a>, <a href="https://github.com/SwiftGen/SwiftGen">SwiftGen</a>,
<a href="https://github.com/yonaskolb/XcodeGen">XcodeGen</a> &amp; used in <a href="https://github.com/jpsim/SourceKitten">SourceKitten</a> to parse Swift
Package Manager build manifests. So if you've wanted to add YAML configuration
files to your Swift CLI, or want to interoperate with other tools that process
YAML, I encourage you to give Yams a try.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Building a serverless password-protected photo gallery]]></title>
    <link href="https://jpsim.com/awspics/"/>
    <updated>2017-07-04T06:14:00+00:00</updated>
    <id>https://jpsim.com/awspics</id>
    <content type="html"><![CDATA[<p>Building a serverless photo gallery?
<em>Easy!</em>
Password-protecting that without adding servers?
<em>Surprisingly much more complex.</em></p>

<p><img src="https://github.com/jpsim/AWSPics/raw/master/assets/awspics.gif" alt="" /></p>

<h2>Goals</h2>

<blockquote><p>Host a self-contained, declarative infrastructure, password-protected,
data-driven static photo gallery to share personal pictures with friends and
family, without needing to run, maintain (or pay for) servers.</p></blockquote>

<p>With <a href="https://twitter.com/simjp/status/873604866043543552">the recent addition</a>
to our family, I wanted to set up a place where I could share pictures with
our closest friends and family. Facebook wasn't an option because...
<a href="https://daringfireball.net/2017/06/fuck_facebook">yeah</a>,
<a href="http://www.slate.com/articles/technology/data_mine_1/2013/09/facebook_privacy_and_kids_don_t_post_photos_of_your_kids_online.html">because</a>
<a href="http://www.telegraph.co.uk/women/family/i-dont-put-pictures-of-my-children-on-facebook---and-you-shouldn/">many</a>
<a href="http://www.huffingtonpost.com.au/2016/02/07/can-i-post-photos-of-other-peoples-children_n_9184560.html">reasons</a>
<a href="https://medium.com/matter/beware-your-baby-s-face-is-online-and-on-sale-d33ae8cdaa9d">actually</a>.</p>

<p>Most of my family members are on Apple devices, while most of my friends are in
the Google/Android ecosystem. So for day to day sharing of moments, we post a
few pictures to iCloud photo sharing, and a few others to a WhatsApp group. But
neither of these can really serve as the canonical place where we're storing
these photos long-term, because WhatsApp is pretty ephemeral, and the iCloud
photo sharing experience <em>sucks</em> on non-Apple devices.</p>

<h2>Architecture</h2>

<p>Unfortunately there's no "put a password in front of AWS CloudFront" checkbox.
I wish there were, so I wouldn't have had to build this. But there isn't, so it
was time to roll up my sleeves and learn a bit about how modern web
infrastructure is built. Or at least my idea of it.</p>

<p><img src="https://github.com/jpsim/AWSPics/raw/master/assets/architecture.png" alt="" /></p>

<p>There are 7 main components:</p>

<ol>
<li><strong>CloudFront with restricted bucket access</strong> to prevent unauthenticated
access to the site or its pictures.</li>
<li><strong>Login lambda function</strong> to validate authentication and sign cookies to
allow access to restricted buckets.</li>
<li><strong>Source S3 bucket</strong> to store original pictures and metadata driving the
site.</li>
<li><strong>Resized S3 bucket</strong> to store resized versions of the original pictures.</li>
<li><strong>Web S3 bucket</strong> to store the static website generated from the data in the
source bucket.</li>
<li><strong>Resize lambda function</strong> to automatically resize images added to the source
S3 bucket and store them in the resized S3 bucket.</li>
<li><strong>Site builder lambda function</strong> to automatically rebuild the static website
when changes are made to the source S3 bucket.</li>
</ol>


<h2>Can it be simplified?</h2>

<p>Of course. There are a few ways I can think of to simplify this, but the
tradeoffs aren't worthwhile IMO.</p>

<ol>
<li><strong>Resize images on demand.</strong> Rather than resize all the images when they're
first added to the source bucket, the resize lambda could be exposed via a
CloudFront origin. However, since the static site only really uses two
image sizes, and that lambda functions have a significant
<a href="https://serverless.com/blog/keep-your-lambdas-warm/">cold start penalty</a>,
it's much more efficient to just precompute the resized images.</li>
<li><strong>Consolidate buckets.</strong>  Rather than separate source, resized &amp; web buckets,
they could just be in a single bucket. However, this would just shift the
complexity a bit since the stack would then need to filter new object
notifications to know which function to invoke. Plus, right now to back up
the valuable content, I just need to periodically mirror the source bucket,
rather than all the derivative data in the resized and web buckets.</li>
</ol>


<h2>Problems?</h2>

<p>I have a few annoyances with this setup as-is.</p>

<p>One is that only the resized bucket triggers the site builder function. That
means that any other object modified in the source bucket, such as the
<code>metadata.yml</code> files that include album comments, don't trigger a site build.</p>

<p>Another related problem is that for every new image in the source bucket, two
are created in the resized bucket, each one invoking the site builder function.
Not only that, but if I upload an album with lots of pictures all in one shot,
the site will be rebuilt twice for each picture! üôÄ</p>

<p>Unfortunately, S3 buckets can only have a
<a href="https://stackoverflow.com/q/31471178/373262">single notification per event type</a>,
so we can't trigger both the resize and site builder functions when new objects
are created on the source bucket.</p>

<p>I think the solution here would involve
<a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html">publishing S3 events to SNS or SQS</a>
and "debouncing" the site builder lambda, but again... <strong>#complexity</strong>.</p>

<p><img src="https://media.giphy.com/media/mYqaRkXyoGbcY/giphy.gif" alt="" /></p>

<p>I'll probably do this eventually, but I'm in no hurry.</p>

<h2>Code</h2>

<p>I've open sourced the entire AWS stack on GitHub over at
<a href="https://github.com/jpsim/AWSPics">jpsim/AWSPics</a>. I'm also hosting a demo
site over at <a href="https://awspics.net">awspics.net</a> (use "username"/"password" as
credentials to check it out).</p>

<p>There's a (long) video walkthrough <a href="https://youtu.be/010AGcY4uoE">on YouTube</a>
too, if that's useful to follow along.</p>

<h2>Closing Thoughts</h2>

<p>Overall, I really enjoyed stepping outside my comfort zone of native
Swift/ObjC/C++ programming, learning a TON about several AWS services and
ultimately meeting my goal of setting up a private photo gallery.</p>

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">If you think incremental Xcode builds take a while, try deploying changes to an AWS CloudFormation stack üòÖ <a href="https://t.co/yTPsqarT92">pic.twitter.com/yTPsqarT92</a></p>&mdash; JP Simard (@simjp) <a href="https://twitter.com/simjp/status/881640404851884033">July 2, 2017</a></blockquote>


<script async src="https://jpsim.com//platform.twitter.com/widgets.js" charset="utf-8"></script>

]]></content>
  </entry>
  
</feed>
