<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: projects | JP Simard]]></title>
  <link href="https://jpsim.com/categories/projects/atom.xml" rel="self"/>
  <link href="https://jpsim.com/"/>
  <updated>2026-01-02T19:54:14+00:00</updated>
  <id>https://jpsim.com/</id>
  <author>
    <name><![CDATA[JP Simard]]></name>
    <email><![CDATA[jp@jpsim.com]]></email>
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Building an NFC Music Box]]></title>
    <link href="https://jpsim.com/building-an-nfc-music-box/"/>
    <updated>2023-03-31T11:00:00+00:00</updated>
    <id>https://jpsim.com/building-an-nfc-music-box</id>
    <content type="html"><![CDATA[<p>My son and I built an NFC-based music player so he can play the music he
wants, develop his own musical taste but most of all so that he and I
could have something fun to build together.</p>

<p>Kind of like a modern day record player, but infinitely more extensible,
and with each album costing 1/100th the price of a vinyl record.</p>

<p>Here it is in action (<em>music starts 9 seconds into the video, set your
volume to low</em>):</p>

<iframe title="vimeo-player" src="https://player.vimeo.com/video/813573763?h=6b3ebec0bc" width="640" height="360" frameborder="0" allowfullscreen></iframe>


<p><br /></p>

<p>And here's how we built it:</p>

<h2>Materials</h2>

<table>
<thead>
<tr>
<th><br /></th>
<th><br /></th>
<th><br /></th>
<th><br /></th>
<th><br /></th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="/images/posts/musicbox/echo.jpg" alt="" /></td>
<td><img src="/images/posts/musicbox/case.jpg" alt="" /></td>
<td><img src="/images/posts/musicbox/pi.jpg" alt="" /></td>
<td><img src="/images/posts/musicbox/nfc_module.jpg" alt="" /></td>
<td><img src="/images/posts/musicbox/nfc_cards.jpg" alt="" /></td>
</tr>
</tbody>
</table>


<ul>
<li><a href="https://www.amazon.com/dp/B08YT3BWMP">Amazon Echo Dot, $20</a>:
For the price, this is a surprisingly hackable, portable speaker. I would have
prefered something without any microphones at all, but I <em>mostly</em> trust the
hardware mute switch.</li>
<li><a href="https://www.etsy.com/listing/739034156">Case, $20</a>: Beautiful
retro-style case for the Raspberry Pi. The top compartment is made to
house a fan, but this is where I put the NFC module instead. The folks
who make this at <a href="https://twitter.com/theC4Labs">C4 Labs</a> were <a href="https://twitter.com/simjp/status/1259980961665576960">super nice</a>.
We broke a piece when we started building the box and they sent a
replacement part right away.</li>
<li><a href="https://www.raspberrypi.org/products/raspberry-pi-4-model-b/">Raspberry Pi, $35</a>:
I chose to go with a Raspberry Pi because they're cheap, have good
compatible NFC modules, lots of case options on Etsy and easy to
develop on.</li>
<li><a href="https://www.amazon.com/dp/B01CSTW0IA">NFC Module, $5</a>: I can't
believe this thing is just $5. Works great. Lots of good tutorials on
connecting this to a Raspberry Pi.</li>
<li><a href="https://store.gototags.com/nfc-pvc-card-ntag213/">NFC Cards, $29</a>:
I got 100 cards at $0.29 each.</li>
</ul>


<p>In total, this adds up to $109, but in my case I already had a Raspberry
Pi and an Echo Dot lying around that I could repurpose for this, so it
cost me closer to $55.</p>

<h2>Services</h2>

<p>I'm already an Apple Music subscriber, so using that as the music source
was the cheapest solution, although this setup would work with any music
service that works with Amazon Echo: Spotify, Amazon Music, TuneIn,
CloudPlayer, Deezer, iHeartRadio.</p>

<p>In the exploration phase for this project, I wanted to buy DRM-free
music, wire a speaker directly to the Raspberry Pi and play it locally.
However, that would have meant that adding music later would be a much
more tedious task. Plus it's hard to find DRM-free music these days, and
when you do find what you want it ends up being pricey. Especially
compared to the convenience and cost of today's streaming options.</p>

<p>I also explored using HomePods as the speakers (see addendum below).</p>

<p>The biggest downside to the current streaming approach is that there's a
5-10 second delay after tapping a card and music starting.</p>

<h2>Assembly</h2>

<table>
<thead>
<tr>
<th><br /></th>
<th><br /></th>
<th><br /></th>
</tr>
</thead>
<tbody>
<tr>
<td><img src="/images/posts/musicbox/assembly1.jpg" alt="Assembly #1" /></td>
<td><img src="/images/posts/musicbox/assembly2.jpg" alt="Assembly #2" /></td>
<td><img src="/images/posts/musicbox/assembly3.jpg" alt="Assembly #3" /></td>
</tr>
<tr>
<td><img src="/images/posts/musicbox/assembly4.jpg" alt="Assembly #4" /></td>
<td><img src="/images/posts/musicbox/assembly5.jpg" alt="Assembly #5" /></td>
<td><img src="/images/posts/musicbox/assembly6.jpg" alt="Assembly #6" /></td>
</tr>
</tbody>
</table>


<p><br /></p>

<p>I assembled this with my son over a few days in May 2020, and then
finished the setup in September 2021. The actual hardware assembly
probably only took a combined total of 3 hours though.</p>

<h2>Software</h2>

<p><em>Disclaimer: This code is rough, it's suitable for a toy project. I'm
sharing it in case it's useful for others getting started with a similar,
project. I'm not claiming this is beautiful quality code.</em></p>

<h3>Echo Dot Remote Control</h3>

<p>I use the <a href="https://github.com/thorsten-gehrig/alexa-remote-control">alexa-remote-control</a> shell script to
control the Echo Dot. When it's up and running, it couldn't be easier:</p>

<pre><code class="shell">$ alexa_remote_control.sh -e "playmusic:APPLE_MUSIC:The Lion King"
$ alexa_remote_control.sh -e pause
$ alexa_remote_control.sh -e play
$ alexa_remote_control.sh -e vol:15
</code></pre>

<h3>NFC Reader</h3>

<p>I use <a href="https://github.com/pelwell/MFRC522-python">MFRC522-python</a> to
interface with the NFC module.</p>

<p>The Python script that reads cards and plays music looks like this:</p>

<pre><code class="python">import RPi.GPIO as GPIO
import MFRC522
import os
import signal

continue_reading = True
last_seen_card = None
echo_name = "MusicBox Echo"
current_dir = os.path.dirname(os.path.abspath(__file__))
alexa_control_script = os.path.join(current_dir, "alexa_remote_control.sh")

def alexa(command):
    os.system(
        '{alexa_control_script} -d "{echo_name}" -e "{command}"'.format(
            alexa_control_script=alexa_control_script,
            echo_name=echo_name,
            command=command,
        )
    )

def pause():
    alexa("pause")

def play_music(query):
    print(query)
    pause()
    alexa("playmusic:APPLE_MUSIC:{query}".format(query=query))

def set_volume(volume):
    alexa("vol:{volume}".format(volume=volume))

def end_read(signal, frame):
    global continue_reading
    continue_reading = False
    GPIO.cleanup()

signal.signal(signal.SIGINT, end_read)

MIFAREReader = MFRC522.MFRC522()

while continue_reading:
    # Scan for cards
    (status, TagType) = MIFAREReader.MFRC522_Request(MIFAREReader.PICC_REQIDL)

    # If we have a card, continue
    if status == MIFAREReader.MI_OK:
        # Get the UID of the card
        (status, uid) = MIFAREReader.MFRC522_Anticoll()

        # If we have the UID, continue
        if status == MIFAREReader.MI_OK and last_seen_card != uid:
            last_seen_card = uid

            # Print UID
            print("Card UID: %s" % uid)
            if uid == [136, 4, 76, 240, 48]:
                print("PAUSE")
                pause()
            elif uid == [136, 4, 236, 236, 140]:
                set_volume(10)
            elif uid == [136, 4, 247, 42, 81]:
                set_volume(40)
            elif uid == [136, 4, 137, 170, 175]:
                play_music("Star Wars A New Hope Soundtrack")
            elif uid == [136, 4, 148, 191, 167]:
                play_music("Paco de Lucia")
            elif uid == [136, 4, 160, 224, 204]:
                play_music("Dexter Gordon")
            elif uid == [136, 4, 197, 238, 167]:
                play_music("Rainbow Connection")
            elif uid == [136, 4, 2, 220, 82]:
                play_music("Genesis")
            elif uid == [136, 4, 232, 57, 93]:
                # and so on...
</code></pre>

<p>You'll notice that there are special cards for pausing, setting a high
volume and setting a low volume. The rest of the cards map to music.</p>

<p>The main thing I'd like to improve at some point is to avoid hardcoding
a mapping of the card UIDs to music and instead program it using the
companion iPhone app by writing a payload to the card. The iOS side I
know how to do pretty quickly, but I'd have to spend more time to figure
out how to do it on the MFRC522 reader side of things.</p>

<h3>API Server</h3>

<p>There's an API server that can be used to control the music box using
the companion iPhone app.</p>

<pre><code class="python">from flask import Flask, request, jsonify
import os

api = Flask(__name__)
echo_name = "MusicBox Echo"
current_dir = os.path.dirname(os.path.abspath(__file__))
alexa_control_script = os.path.join(current_dir, "alexa_remote_control.sh")

def alexa(command):
    os.system(
        '{alexa_control_script} -d "{echo_name}" -e "{command}"'.format(
            alexa_control_script=alexa_control_script,
            echo_name=echo_name,
            command=command,
        )
    )

@api.route("/pause", methods=["POST"])
def pause():
    alexa("pause")
    return "{}"

@api.route("/play", methods=["POST"])
def play():
    if request.json:
        play_music(request.json["query"])
    else:
        alexa("play")
    return "{}"

@api.route("/play/&lt;query&gt;", methods=["POST"])
def play_music(query):
    print(query)
    pause
    alexa("playmusic:APPLE_MUSIC:{query}".format(query=query))
    return "{}"

@api.route("/volume/&lt;int:volume&gt;", methods=["POST"])
def set_volume(volume):
    alexa("vol:{volume}".format(volume=volume))
    return "{}"

if __name__ == "__main__":
    api.run(host="0.0.0.0")
</code></pre>

<h3>iPhone App</h3>

<p>Of course I made an iPhone app using <a href="https://developer.apple.com/xcode/swiftui/">SwiftUI</a> &amp;
<a href="https://github.com/pointfreeco/swift-composable-architecture">Composable Architecture</a>.</p>

<p>This helps me quickly adjust the volume, play/pause and play specific
music. If my kid falls alseep to music, I can stop it without having to
walk into his room.</p>

<p>Siri intents work too, so I can play/pause music by speaking to Siri
without having to launch the app.</p>

<p><img src="/images/posts/musicbox/app.jpg" alt="" /></p>

<h2>Closed Source</h2>

<p>Hopefully this post is helpful to someone interested in building
something similar. I've posted enough code and details to get you
started, but I won't be open sourcing the whole project because I'm just
not willing to field support questions or feature requests. I'm happy to
answer questions about my experience building this, but I'm sorry I
can't help you figure out why something's not working if you go build
something similar.</p>

<hr />

<h2>Addendum: HomePod Attempt</h2>

<p>At one point I wanted to use a stereo pair of HomePods for this project,
but all my attempts to reverse engineer a way to play music on them from
a Raspberry Pi were fruitless. I tried sniffing the network traffic via
Charles Proxy while playing music from the iOS Music app or even the
Shortcuts app and wasn't able to crack it. I did end up getting it
working using <a href="https://github.com/owntone/owntone-server">forked-daap</a> but this set the HomePods in a
weird state. I don't remember all the details because I gave up on that
approach for two reasons: the first is that I couldn't get it to work
well and the second is that I wanted to keep the HomePods in our living
room while the music box was meant to be in my son's bedroom.</p>

<hr />
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Evaluating SwiftSyntax for use in SwiftLint]]></title>
    <link href="https://jpsim.com/evaluating-swiftsyntax-for-use-in-swiftlint/"/>
    <updated>2018-11-22T14:50:00+00:00</updated>
    <id>https://jpsim.com/evaluating-swiftsyntax-for-use-in-swiftlint</id>
    <content type="html"><![CDATA[<p><strong>tl;dr; Implementing SwiftLint using SwiftSyntax instead of SourceKitten would make it run over 20x slower ðŸ˜­</strong></p>

<p><strong>Update:</strong> Since writing this post, I learnt that SwiftSyntax's upcoming byte tree deserialization mode will speed this up considerably.
I hope to post a follow-up article on this shortly.</p>

<p>I have for some time been looking forward to reimplementing some of <a href="https://github.com/realm/SwiftLint">SwiftLint</a>'s simpler syntax-only rules with <a href="https://github.com/apple/swift-syntax">SwiftSyntax</a>. If you're not familiar with it, the recent <a href="https://nshipster.com/swiftsyntax/">NSHipster article</a> gives a great overview. My motivation for integrating it into SwiftLint was that it would be nice to use an officially maintained library directly to obtain the syntax tree rather than the open source but community-maintained <a href="https://github.com/jpsim/SourceKitten">SourceKitten</a> library. I was also under the false impression that SwiftSyntax would be significantly faster than SourceKit/SourceKitten.</p>

<p>SourceKitten gets its syntax tree by dynamically loading <a href="https://github.com/apple/swift/tree/master/tools/SourceKit">SourceKit</a> and making cross-process XPC calls to a SourceKit daemon. In a typical uncached lint run, SwiftLint spends a significant amount of time waiting on this syntax tree for each file being linted. Because SwiftSyntax is <a href="https://github.com/apple/swift-syntax#building-swiftsyntax-from-master">code-generated</a> from the same syntax definition files as the Swift compiler, I had (incorrectly) assumed that calculating a Swift file's syntax tree using SwiftSyntax was done entirely in-process by the library, which would have lead to significant performance gains by avoiding the cross-process XPC call made by SourceKitten for equivalent functionality.</p>

<p>In reality, SwiftSyntax delegates all parsing &amp; lexing to the <code>swiftc</code> binary, <a href="https://github.com/apple/swift-syntax/blob/0.40200.0/Sources/SwiftSyntax/SwiftSyntax.swift#L100-L101">launching the process</a>, <a href="https://github.com/apple/swift-syntax/blob/0.40200.0/Sources/SwiftSyntax/SwiftSyntax.swift#L102">reading its output from stdout</a> and <a href="https://github.com/apple/swift-syntax/blob/0.40200.0/Sources/SwiftSyntax/SwiftSyntax.swift#L103-L104">deserializing the JSON response</a> into its <code>SourceFileSyntax</code> Swift type. This is repeated for each file being parsed ðŸ˜±.</p>

<p><strong>Launching a new instance of the Swift compiler for each file parsed is orders of magnitude slower than SourceKitten's XPC call to a long-lived SourceKit daemon.</strong></p>

<p>I discovered this after <a href="https://github.com/realm/SwiftLint/pull/2476">reimplementing</a> a very simple SwiftLint rule with a SwiftSyntax-based implementation: <a href="https://github.com/realm/SwiftLint/blob/master/Rules.md#fallthrough">Fallthrough</a>. This opt-in rule is a perfect proof-of-concept for integrating SwiftSyntax into SwiftLint because it literally just finds all occurrences of the <code>fallthrough</code> keyword and reports a violation at that location. I measured the time it took to lint a folder of ~100 Swift files from Lyft's iOS codebase with only the <code>fallthrough</code> rule whitelisted.</p>

<pre><code class="yaml"># .swiftlint.yml configuration file
included:
  - path/to/lint/dir # contains ~100 Swift files
whitelist_rules:
  - fallthrough
</code></pre>

<p>I compiled both SwiftLint from <code>master</code> and again with this <code>fallthrough-swift-syntax</code> branch with <code>swift build -c release</code> and named the binaries <code>swiftlint-master</code> and <code>swiftlint-swift-syntax</code>. I then benchmarked both binaries using the excellent <a href="https://github.com/sharkdp/hyperfine">hyperfine</a> utility.</p>

<pre><code class="shell">$ hyperfine './swiftlint-master lint --quiet --no-cache' './swiftlint-swift-syntax lint --quiet --no-cache'
Benchmark #1: ./swiftlint-master lint --quiet --no-cache
  Time (mean Â± Ïƒ):     231.3 ms Â±   5.1 ms    [User: 130.5 ms, System: 29.2 ms]
  Range (min â€¦ max):   224.3 ms â€¦ 238.3 ms

Benchmark #2: ./swiftlint-swift-syntax lint --quiet --no-cache
  Time (mean Â± Ïƒ):      5.254 s Â±  0.149 s    [User: 20.309 s, System: 23.110 s]
  Range (min â€¦ max):    4.839 s â€¦  5.354 s

Summary
  './swiftlint-master lint --quiet --no-cache' ran
   22.71 Â± 0.82 times faster than './swiftlint-swift-syntax lint --quiet --no-cache'
</code></pre>

<p><strong>The SwiftSyntax version was 22x slower than the existing SourceKitten version</strong></p>

<p><em>Note that I ran SwiftLint with its caching mechanism and logging disabled to accurately measure the time it took just to perform the lint, rather than the overhead from logging or skipping the lint entirely by just returning cached results. Although logging only added 3ms to 10ms in my tests.</em></p>

<hr />

<p>Ultimately, this means SwiftLint will be keeping its SourceKitten-based implementation for the foreseeable future, unless SwiftSyntax removes its reliance on costly compiler invocations and drastically improves its performance. I really hope the Swift team can somehow find a way to move parsing and lexing into SwiftSyntax itself, making the library much more appealing to use.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Introducing Yams 1.0]]></title>
    <link href="https://jpsim.com/introducing-yams-1-dot-0/"/>
    <updated>2018-05-19T11:40:00+00:00</updated>
    <id>https://jpsim.com/introducing-yams-1-dot-0</id>
    <content type="html"><![CDATA[<p><img src="https://raw.githubusercontent.com/jpsim/Yams/master/yams.jpg" alt="Yams: A Sweet &amp; Swifty YAML Parser" /></p>

<p><a href="https://twitter.com/norio_nomura">Norio Nomura</a> and I (ok, honestly mostly Norio ðŸ˜…) have been building a
Swift library for encoding &amp; decoding <a href="http://yaml.org">YAML</a> for the last 18 months and
it's now stable enough to make a 1.0 release and share with the world.</p>

<p>It's called Yams, you can find it on GitHub at <a href="https://github.com/jpsim/Yams">jpsim/Yams</a> and API
docs are located at <a href="https://jpsim.com/Yams">jpsim.com/Yams</a>.</p>

<p>You could say it's a Swift binding for <a href="https://github.com/yaml/libyaml">LibYAML</a> but I'd argue it's
much more than that.</p>

<p>I'm actually very happy with how this library ended up. It plays nicely with
Swift 4's <a href="https://developer.apple.com/documentation/foundation/archives_and_serialization/encoding_and_decoding_custom_types">Codable protocol</a>, meaning that you get fast &amp; type-safe
encoding &amp; decoding that feels right at home in modern Swift.</p>

<p>Here's a simple example of encoding &amp; decoding <code>Codable</code> types:</p>

<pre><code class="swift">import Yams

struct S: Codable {
  var p: String
}

let s = S(p: "test")
let encoder = YAMLEncoder()
let encodedYAML = try encoder.encode(s)
encodedYAML == """
  p: test

  """
let decoder = YAMLDecoder()
let decoded = try decoder.decode(S.self, from: encodedYAML)
s.p == decoded.p
</code></pre>

<p>Alternatively, you can use it to work with Swift scalar &amp; collection types,
which is probably the easiest way to start parsing arbitrary YAML for your
projects. Finally, there's a third mode, which is a Yams-native API that best
translates to how LibYAML works.</p>

<p>This library's been powering a number of popular projects that use YAML for
configuration, like <a href="https://github.com/realm/SwiftLint">SwiftLint</a>, <a href="https://github.com/SwiftGen/SwiftGen">SwiftGen</a>,
<a href="https://github.com/yonaskolb/XcodeGen">XcodeGen</a> &amp; used in <a href="https://github.com/jpsim/SourceKitten">SourceKitten</a> to parse Swift
Package Manager build manifests. So if you've wanted to add YAML configuration
files to your Swift CLI, or want to interoperate with other tools that process
YAML, I encourage you to give Yams a try.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Building a serverless password-protected photo gallery]]></title>
    <link href="https://jpsim.com/awspics/"/>
    <updated>2017-07-04T06:14:00+00:00</updated>
    <id>https://jpsim.com/awspics</id>
    <content type="html"><![CDATA[<p>Building a serverless photo gallery?
<em>Easy!</em>
Password-protecting that without adding servers?
<em>Surprisingly much more complex.</em></p>

<p><img src="https://github.com/jpsim/AWSPics/raw/master/assets/awspics.gif" alt="" /></p>

<h2>Goals</h2>

<blockquote><p>Host a self-contained, declarative infrastructure, password-protected,
data-driven static photo gallery to share personal pictures with friends and
family, without needing to run, maintain (or pay for) servers.</p></blockquote>

<p>With <a href="https://twitter.com/simjp/status/873604866043543552">the recent addition</a>
to our family, I wanted to set up a place where I could share pictures with
our closest friends and family. Facebook wasn't an option because...
<a href="https://daringfireball.net/2017/06/fuck_facebook">yeah</a>,
<a href="http://www.slate.com/articles/technology/data_mine_1/2013/09/facebook_privacy_and_kids_don_t_post_photos_of_your_kids_online.html">because</a>
<a href="http://www.telegraph.co.uk/women/family/i-dont-put-pictures-of-my-children-on-facebook---and-you-shouldn/">many</a>
<a href="http://www.huffingtonpost.com.au/2016/02/07/can-i-post-photos-of-other-peoples-children_n_9184560.html">reasons</a>
<a href="https://medium.com/matter/beware-your-baby-s-face-is-online-and-on-sale-d33ae8cdaa9d">actually</a>.</p>

<p>Most of my family members are on Apple devices, while most of my friends are in
the Google/Android ecosystem. So for day to day sharing of moments, we post a
few pictures to iCloud photo sharing, and a few others to a WhatsApp group. But
neither of these can really serve as the canonical place where we're storing
these photos long-term, because WhatsApp is pretty ephemeral, and the iCloud
photo sharing experience <em>sucks</em> on non-Apple devices.</p>

<h2>Architecture</h2>

<p>Unfortunately there's no "put a password in front of AWS CloudFront" checkbox.
I wish there were, so I wouldn't have had to build this. But there isn't, so it
was time to roll up my sleeves and learn a bit about how modern web
infrastructure is built. Or at least my idea of it.</p>

<p><img src="https://github.com/jpsim/AWSPics/raw/master/assets/architecture.png" alt="" /></p>

<p>There are 7 main components:</p>

<ol>
<li><strong>CloudFront with restricted bucket access</strong> to prevent unauthenticated
access to the site or its pictures.</li>
<li><strong>Login lambda function</strong> to validate authentication and sign cookies to
allow access to restricted buckets.</li>
<li><strong>Source S3 bucket</strong> to store original pictures and metadata driving the
site.</li>
<li><strong>Resized S3 bucket</strong> to store resized versions of the original pictures.</li>
<li><strong>Web S3 bucket</strong> to store the static website generated from the data in the
source bucket.</li>
<li><strong>Resize lambda function</strong> to automatically resize images added to the source
S3 bucket and store them in the resized S3 bucket.</li>
<li><strong>Site builder lambda function</strong> to automatically rebuild the static website
when changes are made to the source S3 bucket.</li>
</ol>


<h2>Can it be simplified?</h2>

<p>Of course. There are a few ways I can think of to simplify this, but the
tradeoffs aren't worthwhile IMO.</p>

<ol>
<li><strong>Resize images on demand.</strong> Rather than resize all the images when they're
first added to the source bucket, the resize lambda could be exposed via a
CloudFront origin. However, since the static site only really uses two
image sizes, and that lambda functions have a significant
<a href="https://serverless.com/blog/keep-your-lambdas-warm/">cold start penalty</a>,
it's much more efficient to just precompute the resized images.</li>
<li><strong>Consolidate buckets.</strong>  Rather than separate source, resized &amp; web buckets,
they could just be in a single bucket. However, this would just shift the
complexity a bit since the stack would then need to filter new object
notifications to know which function to invoke. Plus, right now to back up
the valuable content, I just need to periodically mirror the source bucket,
rather than all the derivative data in the resized and web buckets.</li>
</ol>


<h2>Problems?</h2>

<p>I have a few annoyances with this setup as-is.</p>

<p>One is that only the resized bucket triggers the site builder function. That
means that any other object modified in the source bucket, such as the
<code>metadata.yml</code> files that include album comments, don't trigger a site build.</p>

<p>Another related problem is that for every new image in the source bucket, two
are created in the resized bucket, each one invoking the site builder function.
Not only that, but if I upload an album with lots of pictures all in one shot,
the site will be rebuilt twice for each picture! ðŸ™€</p>

<p>Unfortunately, S3 buckets can only have a
<a href="https://stackoverflow.com/q/31471178/373262">single notification per event type</a>,
so we can't trigger both the resize and site builder functions when new objects
are created on the source bucket.</p>

<p>I think the solution here would involve
<a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html">publishing S3 events to SNS or SQS</a>
and "debouncing" the site builder lambda, but again... <strong>#complexity</strong>.</p>

<p><img src="https://media.giphy.com/media/mYqaRkXyoGbcY/giphy.gif" alt="" /></p>

<p>I'll probably do this eventually, but I'm in no hurry.</p>

<h2>Code</h2>

<p>I've open sourced the entire AWS stack on GitHub over at
<a href="https://github.com/jpsim/AWSPics">jpsim/AWSPics</a>. I'm also hosting a demo
site over at <a href="https://awspics.net">awspics.net</a> (use "username"/"password" as
credentials to check it out).</p>

<p>There's a (long) video walkthrough <a href="https://youtu.be/010AGcY4uoE">on YouTube</a>
too, if that's useful to follow along.</p>

<h2>Closing Thoughts</h2>

<p>Overall, I really enjoyed stepping outside my comfort zone of native
Swift/ObjC/C++ programming, learning a TON about several AWS services and
ultimately meeting my goal of setting up a private photo gallery.</p>

<blockquote class="twitter-tweet" data-lang="en"><p lang="en" dir="ltr">If you think incremental Xcode builds take a while, try deploying changes to an AWS CloudFormation stack ðŸ˜… <a href="https://t.co/yTPsqarT92">pic.twitter.com/yTPsqarT92</a></p>&mdash; JP Simard (@simjp) <a href="https://twitter.com/simjp/status/881640404851884033">July 2, 2017</a></blockquote>


<script async src="https://jpsim.com//platform.twitter.com/widgets.js" charset="utf-8"></script>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[JPSThumbnailAnnotation updated for iOS 7]]></title>
    <link href="https://jpsim.com/jpsthumbnailannotation-updated-for-ios-7/"/>
    <updated>2014-03-27T11:55:00+00:00</updated>
    <id>https://jpsim.com/jpsthumbnailannotation-updated-for-ios-7</id>
    <content type="html"><![CDATA[<p>I've just given a fresh coat of iOS7-flavoured paint to my most popular open-source library: JPSThumbnailAnnotation.</p>

<p>Check it out on <a href="https://github.com/jpsim/JPSThumbnailAnnotation">GitHub</a> or just add it to CocoaPods: <code>pod 'JPSThumbnailAnnotation'</code>.</p>

<p><img src="https://github.com/jpsim/JPSThumbnailAnnotation/raw/master/screenshots2.jpg" alt="Screenshots" /></p>

<p>I decided not to maintain the iOS 6 <em>style</em>, but iOS 6 is still supported.</p>

<p>By the way, this is what the old style looked like. It's still in git, if you need to find it for some reason.</p>

<p><img src="https://github.com/jpsim/JPSThumbnailAnnotation/raw/master/screenshots.jpg" alt="Old Screenshots" /></p>
]]></content>
  </entry>
  
</feed>
